#!/usr/bin/env python3
"""Summarize PBS job CPU and memory usage for a user or single job."""

import argparse
import getpass
import os
import re
import shutil
import subprocess
import sys
from typing import Optional, Dict, Any, List

# ---------- Regex patterns for qstat -fx output ----------
RE_CPUT    = re.compile(r"^\s*resources_used\.cput\s*=\s*(\S+)", re.M)
RE_WALL    = re.compile(r"^\s*resources_used\.walltime\s*=\s*(\S+)", re.M)
RE_MEMU    = re.compile(r"^\s*resources_used\.mem\s*=\s*(\S+)", re.M)
RE_VMEMU   = re.compile(r"^\s*resources_used\.vmem\s*=\s*(\S+)", re.M)
RE_NCPUS   = re.compile(r"^\s*Resource_List\.ncpus\s*=\s*(\d+)", re.M)
RE_NGPUS   = re.compile(r"^\s*Resource_List\.ngpus\s*=\s*(\d+)", re.M)
RE_NGPU    = re.compile(r"^\s*Resource_List\.ngpu\s*=\s*(\d+)", re.M)
RE_MEMREQ  = re.compile(r"^\s*Resource_List\.mem\s*=\s*(\S+)", re.M)
RE_VMEMRQ  = re.compile(r"^\s*Resource_List\.(?:vmem|pvmem)\s*=\s*(\S+)", re.M)
RE_SELECT  = re.compile(r"^\s*Resource_List\.select\s*=\s*([^\s]+)", re.M)
RE_OWNER   = re.compile(r"^\s*Job_Owner\s*=\s*([^@]+)@", re.M)
RE_STATE   = re.compile(r"^\s*job_state\s*=\s*(\S+)", re.M)
RE_NAME    = re.compile(r"^\s*Job_Name\s*=\s*(\S+)", re.M)
RE_JOBID   = re.compile(r"^\s*Job Id:\s*(\S+)", re.M)
RE_EXECHOST = re.compile(r"^\s*exec_host\s*=\s*(\S.*(?:\n\s+\S.*)*)", re.M)

# ---------- Utilities ----------
def hms_to_seconds(hms: str) -> int:
    """Convert HHHH:MM:SS (hours may exceed 24) to seconds."""
    parts = [int(x) for x in hms.strip().split(":")]
    sec, mult = 0, 1
    for x in reversed(parts):
        sec += x * mult
        mult *= 60
    return sec

def parse_size_to_bytes(s: str) -> Optional[int]:
    """Parse '123456kb', '64gb', '1.5GB', '2048' to bytes (IEC 1024)."""
    m = re.match(r"^([0-9]+(?:\.[0-9]+)?)\s*([a-zA-Z]*)$", s.strip())
    if not m:
        return None
    val = float(m.group(1))
    unit = (m.group(2) or "").lower()
    table = {
        "":1, "b":1,
        "k":1024, "kb":1024, "kib":1024,
        "m":1024**2, "mb":1024**2, "mib":1024**2,
        "g":1024**3, "gb":1024**3, "gib":1024**3,
        "t":1024**4, "tb":1024**4, "tib":1024**4,
        "p":1024**5, "pb":1024**5, "pib":1024**5,
    }
    mult = table.get(unit)
    return int(val * mult) if mult else None

def fmt_bytes(n: Optional[int]) -> str:
    """Human-readable IEC format."""
    if not n or n <= 0:
        return "n/a"
    for unit, m in [("PiB",1024**5),("TiB",1024**4),("GiB",1024**3),("MiB",1024**2),("KiB",1024)]:
        if n >= m:
            return f"{n/m:.2f} {unit}"
    return f"{n} B"

def secs_to_h(s: Optional[int]) -> str:
    return "n/a" if s is None else f"{s/3600:.2f}"

def pct_str(x: Optional[float]) -> str:
    return f"{x*100:.2f}%" if x is not None else "n/a"

def run(cmd: List[str]) -> str:
    return subprocess.check_output(cmd, text=True, errors="ignore")

def non_negative_int(val: str) -> int:
    n = int(val)
    if n < 0:
        raise argparse.ArgumentTypeError("must be >= 0")
    return n

# ---------- select=... parsing ----------
def parse_total_ncpus_from_select(select_str: str) -> Optional[int]:
    total = 0
    for chunk in select_str.split("+"):
        fields = chunk.split(":")
        count, idx = (int(fields[0]), 1) if fields and fields[0].isdigit() else (1, 0)
        ncpus = 0
        for f in fields[idx:]:
            if f.startswith("ncpus="):
                try:
                    ncpus = int(f.split("=",1)[1])
                except ValueError:
                    pass
        total += count * ncpus
    return total or None

def parse_total_mem_from_select(select_str: str) -> Optional[int]:
    total, seen = 0, False
    for chunk in select_str.split("+"):
        fields = chunk.split(":")
        count, idx = (int(fields[0]), 1) if fields and fields[0].isdigit() else (1, 0)
        mem_bytes = None
        for f in fields[idx:]:
            if f.startswith("mem="):
                mem_bytes = parse_size_to_bytes(f.split("=",1)[1])
                seen = True
        if mem_bytes:
            total += count * mem_bytes
    return total if seen else None

def parse_total_gpus_from_select(select_str: str) -> Optional[int]:
    total, seen = 0, False
    for chunk in select_str.split("+"):
        fields = chunk.split(":")
        count, idx = (int(fields[0]), 1) if fields and fields[0].isdigit() else (1, 0)
        ngpu_val = None
        for f in fields[idx:]:
            if f.startswith("ngpus=") or f.startswith("ngpu=") or f.startswith("gpus="):
                try:
                    ngpu_val = int(f.split("=",1)[1])
                    seen = True
                except ValueError:
                    pass
        if ngpu_val:
            total += count * ngpu_val
    return total if seen else None

# ---------- Single job summarization ----------
def summarize_job_from_block(block: str) -> Dict[str, Any]:
    jobid = (RE_JOBID.search(block).group(1) if RE_JOBID.search(block) else "unknown")
    name  = (RE_NAME.search(block).group(1) if RE_NAME.search(block) else "")
    state = (RE_STATE.search(block).group(1) if RE_STATE.search(block) else "?")

    nodes = None
    m_exec = RE_EXECHOST.search(block)
    if m_exec:
        hosts_raw = re.sub(r"\s+", "", m_exec.group(1))
        host_list: List[str] = []
        for chunk in hosts_raw.split("+"):
            host = chunk.split("/")[0]
            if host and host not in host_list:
                host_list.append(host)
        nodes = ",".join(host_list) if host_list else None

    cput_s = hms_to_seconds(RE_CPUT.search(block).group(1)) if RE_CPUT.search(block) else None
    wall_s = hms_to_seconds(RE_WALL.search(block).group(1)) if RE_WALL.search(block) else None

    ncpus_m = RE_NCPUS.search(block)
    ncpus = int(ncpus_m.group(1)) if ncpus_m else None
    if ncpus is None and RE_SELECT.search(block):
        ncpus = parse_total_ncpus_from_select(RE_SELECT.search(block).group(1))

    ngpus = None
    if RE_NGPUS.search(block):
        ngpus = int(RE_NGPUS.search(block).group(1))
    elif RE_NGPU.search(block):
        ngpus = int(RE_NGPU.search(block).group(1))
    elif RE_SELECT.search(block):
        ngpus = parse_total_gpus_from_select(RE_SELECT.search(block).group(1))

    used_mem_b  = parse_size_to_bytes(RE_MEMU.search(block).group(1))  if RE_MEMU.search(block)  else None
    used_vmem_b = parse_size_to_bytes(RE_VMEMU.search(block).group(1)) if RE_VMEMU.search(block) else None

    req_mem_b = parse_size_to_bytes(RE_MEMREQ.search(block).group(1)) if RE_MEMREQ.search(block) else None
    if req_mem_b is None and RE_SELECT.search(block):
        req_mem_b = parse_total_mem_from_select(RE_SELECT.search(block).group(1))

    req_vmem_b = parse_size_to_bytes(RE_VMEMRQ.search(block).group(1)) if RE_VMEMRQ.search(block) else None

    avg_used_cpus = (cput_s / wall_s) if (cput_s is not None and wall_s and wall_s > 0) else None
    cpu_eff = (avg_used_cpus / ncpus) if (avg_used_cpus is not None and ncpus) else None
    mem_eff = (used_mem_b / req_mem_b) if (used_mem_b and req_mem_b and req_mem_b > 0) else None
    vmem_eff = (used_vmem_b / req_vmem_b) if (used_vmem_b and req_vmem_b and req_vmem_b > 0) else None

    return {
        "jobid": jobid, "name": name, "state": state, "nodes": nodes,
        "ncpus": ncpus, "wall_s": wall_s, "cput_s": cput_s,
        "ngpus": ngpus,
        "avg_used_cpus": avg_used_cpus, "cpu_eff": cpu_eff,
        "used_mem_b": used_mem_b, "req_mem_b": req_mem_b, "mem_eff": mem_eff,
        "used_vmem_b": used_vmem_b, "req_vmem_b": req_vmem_b, "vmem_eff": vmem_eff,
    }

def summarize_job(jobid: str) -> Dict[str, Any]:
    blk = run(["qstat", "-fx", jobid])
    return summarize_job_from_block(blk)


def is_pbs_finished_state(state: str) -> bool:
    return state.strip().upper() in {"X", "F"}


def jobid_key(jobid: Optional[str]) -> str:
    if not jobid:
        return ""
    return jobid.split(".", 1)[0]


# ---------- Fast bulk path ----------
def split_qstat_f_blocks(blob: str) -> List[str]:
    """Split a single 'qstat -f ...' response into per-job blocks."""
    lines = blob.splitlines()
    idxs = [i for i, ln in enumerate(lines) if ln.strip().startswith("Job Id:")]
    blocks = []
    for i, start in enumerate(idxs):
        end = idxs[i+1] if i+1 < len(idxs) else len(lines)
        blocks.append("\n".join(lines[start:end]))
    return blocks


def summarize_all_jobs_bulk(user: str, include_finished: bool) -> list[dict]:
    # Try with -u first; if server disallows, fall back to all and filter by owner.
    variants = [
        ["qstat"] + (["-x"] if include_finished else []) + ["-f", "-u", user, "-t"],
        ["qstat"] + (["-x"] if include_finished else []) + ["-f", "-t"],
    ]
    rows = []
    for args in variants:
        try:
            blob = run(args)
        except subprocess.CalledProcessError:
            continue
        blocks = split_qstat_f_blocks(blob)
        cand = []
        for b in blocks:
            m = RE_OWNER.search(b)
            if not m:
                continue
            owner = m.group(1)  # Job_Owner = <user>@host
            if owner == user:
                cand.append(summarize_job_from_block(b))
        if cand:
            rows = cand
            break
    return rows


# ---------- Compat path (per job) ----------
def list_user_jobids(user: str, include_finished: bool=False) -> list[str]:
    args = []
    if include_finished:
        args.append("-x")
    args += ["-u", user, "-t"]
    out = run(["qstat", *args])
    jobids = []
    for line in out.splitlines():
        line = line.strip()
        if not line or line.lower().startswith("job id") or line.startswith("-"):
            continue
        tok = line.split()[0]
        # Accept: 12345 | 12345.server | 12345[1] | 12345[1].server
        if re.match(r"^\d+(?:\[[^\]]+\])?(?:\.\S+)?$", tok):
            jobids.append(tok)
    return jobids


def summarize_all_jobs_compat(user: str, include_finished: bool) -> List[Dict[str, Any]]:
    jobids = list_user_jobids(user, include_finished=include_finished)
    rows = []
    for jid in jobids:
        try:
            rows.append(summarize_job(jid))
        except subprocess.CalledProcessError:
            continue
    return rows


def summarize_jobs_with_finished_limit_fetch(user: str, mode: str, finished_limit: int) -> List[Dict[str, Any]]:
    if mode == "bulk":
        active_rows = summarize_all_jobs_bulk(user, include_finished=False)
    else:
        active_rows = summarize_all_jobs_compat(user, include_finished=False)

    if finished_limit <= 0:
        return active_rows

    active_keys = {jobid_key(r.get("jobid")) for r in active_rows}
    finished_rows: List[Dict[str, Any]] = []
    for jid in list_user_jobids(user, include_finished=True):
        if jobid_key(jid) in active_keys:
            continue
        try:
            row = summarize_job(jid)
        except subprocess.CalledProcessError:
            continue
        if not is_pbs_finished_state(row.get("state") or ""):
            continue
        finished_rows.append(row)
        if len(finished_rows) >= finished_limit:
            break
    return active_rows + finished_rows

# ---------- Output ----------
def render_table(rows: List[Dict[str,Any]], name_max: int) -> None:
    cols = ["JOBID","STATE","NAME","NODES","NCPUS","NGPUS","WALL(h)","CPUT(h)","avgCPU","CPUeff","memUsed","memReq","memEff"]
    w = {c: len(c) for c in cols}
    table = []
    for r in rows:
        name = r.get("name") or ""
        if name_max > 0 and len(name) > name_max:
            name = name[:max(0,name_max-1)] + "â€¦"
        row = {
            "JOBID":  r["jobid"],
            "STATE":  r["state"],
            "NAME":   name,
            "NODES":  r.get("nodes") or "n/a",
            "NCPUS":  str(r["ncpus"] if r["ncpus"] is not None else "n/a"),
            "NGPUS":  str(r["ngpus"] if r.get("ngpus") is not None else "n/a"),
            "WALL(h)": secs_to_h(r["wall_s"]),
            "CPUT(h)": secs_to_h(r["cput_s"]),
            "avgCPU": f"{r['avg_used_cpus']:.2f}" if r["avg_used_cpus"] is not None else "n/a",
            "CPUeff": pct_str(r["cpu_eff"]),
            "memUsed": fmt_bytes(r["used_mem_b"]),
            "memReq":  fmt_bytes(r["req_mem_b"]),
            "memEff":  pct_str(r["mem_eff"]),
        }
        for k,v in row.items():
            w[k] = max(w[k], len(str(v)))
        table.append(row)
    hdr = "  ".join(f"{c:<{w[c]}}" for c in cols)
    print(hdr)
    print("-" * len(hdr))
    for row in table:
        print("  ".join(f"{row[c]:<{w[c]}}" for c in cols))

def write_csv(rows: List[Dict[str,Any]], path: str) -> None:
    import csv
    fields = [
        "jobid","name","state","nodes","ncpus","ngpus","wall_s","cput_s","avg_used_cpus","cpu_eff",
        "used_mem_b","used_mem_gb","req_mem_b","req_mem_gb","mem_eff",
        "used_vmem_b","used_vmem_gb","req_vmem_b","req_vmem_gb","vmem_eff",
    ]
    f = sys.stdout if path == "-" else open(path, "w", newline="")
    with f:
        w = csv.DictWriter(f, fieldnames=fields, delimiter="\t")
        w.writeheader()
        for r in rows:
            row = {
                "jobid": r.get("jobid"),
                "name": r.get("name"),
                "state": r.get("state"),
                "nodes": r.get("nodes"),
                "ncpus": r.get("ncpus"),
                "ngpus": r.get("ngpus"),
                "wall_s": r.get("wall_s"),
                "cput_s": r.get("cput_s"),
                "avg_used_cpus": r.get("avg_used_cpus"),
                "cpu_eff": r.get("cpu_eff"),
                "used_mem_b": r.get("used_mem_b"),
                "req_mem_b": r.get("req_mem_b"),
                "mem_eff": r.get("mem_eff"),
                "used_vmem_b": r.get("used_vmem_b"),
                "req_vmem_b": r.get("req_vmem_b"),
                "vmem_eff": r.get("vmem_eff"),
            }

            if row["avg_used_cpus"] is not None:
                row["avg_used_cpus"] = round(row["avg_used_cpus"], 2)

            for field in ["cpu_eff", "mem_eff", "vmem_eff"]:
                if row.get(field) is not None:
                    row[field] = round(row[field], 2)

            for src, dest in [
                ("used_mem_b", "used_mem_gb"),
                ("req_mem_b", "req_mem_gb"),
                ("used_vmem_b", "used_vmem_gb"),
                ("req_vmem_b", "req_vmem_gb"),
            ]:
                val = row.get(src)
                row[dest] = round((val / (1024**3)), 2) if val is not None else None

            w.writerow({k: row.get(k) for k in fields})

def aggregate(rows: List[Dict[str,Any]]) -> Dict[str, Any]:
    import math
    def mean(xs):
        xs = [x for x in xs if x is not None and not (isinstance(x,float) and (math.isnan(x) or math.isinf(x)))]
        return sum(xs)/len(xs) if xs else float("nan")
    peak_mem = None
    mem_vals = [r.get("used_mem_b") for r in rows if r.get("used_mem_b")]
    if mem_vals:
        peak_mem = max(mem_vals)
    unique_nodes: set[str] = set()
    state_counts: Dict[str, int] = {}
    for r in rows:
        nodes_field = r.get("nodes")
        if nodes_field:
            for node in nodes_field.split(","):
                node = node.strip()
                if node:
                    unique_nodes.add(node)

        state = (r.get("state") or "").strip() or "?"
        state_counts[state] = state_counts.get(state, 0) + 1

    other_states = {k: v for k, v in state_counts.items() if k not in {"X", "R", "Q"}}
    return {
        "jobs": len(rows),
        "avg_CPUeff_%": mean([r["cpu_eff"]*100 for r in rows if r.get("cpu_eff") is not None]),
        "avg_avgCPU":   mean([r["avg_used_cpus"] for r in rows if r.get("avg_used_cpus") is not None]),
        "avg_memEff_%": mean([r["mem_eff"]*100 for r in rows if r.get("mem_eff") is not None]),
        "max_mem_b": peak_mem,
        "unique_nodes": len(unique_nodes),
        "state_counts": state_counts,
        "state_other_total": sum(other_states.values()),
        "state_other_breakdown": other_states,
    }


def limit_finished_rows(rows: List[Dict[str, Any]], finished_limit: Optional[int]) -> List[Dict[str, Any]]:
    if finished_limit is None:
        return rows
    kept_finished = 0
    filtered: List[Dict[str, Any]] = []
    for row in rows:
        is_finished = is_pbs_finished_state(row.get("state") or "")
        if is_finished:
            if kept_finished >= finished_limit:
                continue
            kept_finished += 1
        filtered.append(row)
    return filtered

# ---------- CLI ----------
def main():
    ap = argparse.ArgumentParser(
        description="PBS job CPU & Memory statistics (single job or all jobs for a user)."
    )
    ap.add_argument("--job", help="Job ID to summarize (default: $PBS_JOBID). Returns the current job statistics if valid $PBS JOBID is found and --user argument was not specified explicitly")
    ap.add_argument("--user", help="Summarize all jobs of USER (default: current user). The --user argument takes precedence over the --job value")
    ap.add_argument("--include-finished", action="store_true", help="Include finished jobs (qstat -x)")
    ap.add_argument(
        "--finished-limit",
        type=non_negative_int,
        metavar="N",
        help="With --include-finished, show at most N finished jobs (state X/F). Active jobs are always shown.",
    )
    ap.add_argument(
        "--finished-limit-strategy",
        choices=["post", "fetch"],
        default="post",
        help="How to apply --finished-limit: post=fetch all then trim (default), fetch=fetch active + up to N finished jobs.",
    )
    ap.add_argument("--mode", choices=["bulk","compat"], default="bulk",
    help="Bulk mode: one qstat -f for all jobs; auto-fallback to compat if it fails.\n Compat mode: one qstat -fx per job (slower but widely compatible)"
    )
    ap.add_argument("--csv", metavar="PATH", help='Write CSV to PATH (use "-" for stdout)')
    ap.add_argument("--name-max", type=int, default=30, help="Max width for job name column; 0=disable truncation (default: 30)")
    args = ap.parse_args()

    if args.finished_limit is not None and not args.include_finished:
        ap.error("--finished-limit requires --include-finished")

    if not shutil.which("qstat"):
        print("ERROR: qstat not found in PATH.", file=sys.stderr)
        sys.exit(1)

    rows: List[Dict[str,Any]] = []

    jobid = args.job or os.environ.get("PBS_JOBID")
    default_user = os.environ.get("PBS_O_LOGNAME") or os.environ.get("USER") or getpass.getuser()

    # single-job path
    if args.user is None and jobid:
        rows.append(summarize_job(jobid))
    else:
        # multi-job path
        user = args.user or default_user
        if not user:
            print("Provide --job JOBID or run inside PBS with $PBS_JOBID set, or use --user USER.", file=sys.stderr)
            sys.exit(2)
        used_fast = False
        if args.include_finished and args.finished_limit is not None and args.finished_limit_strategy == "fetch":
            try:
                rows = summarize_jobs_with_finished_limit_fetch(
                    user=user, mode=args.mode, finished_limit=args.finished_limit
                )
            except subprocess.CalledProcessError:
                if args.mode == "bulk":
                    rows = summarize_all_jobs_bulk(user, include_finished=True)
                else:
                    rows = summarize_all_jobs_compat(user, include_finished=True)
            rows = limit_finished_rows(rows, args.finished_limit)
        else:
            if args.mode == "bulk":
                try:
                    rows = summarize_all_jobs_bulk(user, include_finished=args.include_finished)
                    used_fast = True
                except subprocess.CalledProcessError:
                    rows = []
                except Exception:
                    rows = []
            if not rows and args.mode == "compat":
                rows = summarize_all_jobs_compat(user, include_finished=args.include_finished)
            rows = limit_finished_rows(rows, args.finished_limit)

    # output
    render_table(rows, name_max=args.name_max)
    agg = aggregate(rows)
    print("\nSummary:")
    print(f"  jobs:         {agg['jobs']}")
    print(f"  unique nodes: {agg['unique_nodes']}")
    state_counts = agg.get("state_counts", {})
    r_count = state_counts.get("R", 0)
    q_count = state_counts.get("Q", 0)
    x_count = state_counts.get("X", 0)
    other_total = agg.get("state_other_total", 0)
    print(f"  states:       R={r_count}  Q={q_count}  X={x_count}  other={other_total}")
    other_breakdown = agg.get("state_other_breakdown") or {}
    if other_breakdown:
        detail = ", ".join(f"{k}={v}" for k, v in sorted(other_breakdown.items()))
        print(f"    other breakdown: {detail}")
    if agg['avg_CPUeff_%'] == agg['avg_CPUeff_%']:
        print(f"  mean CPUeff:  {agg['avg_CPUeff_%']:.2f}%")
    if agg['avg_avgCPU'] == agg['avg_avgCPU']:
        print(f"  mean avgCPU:  {agg['avg_avgCPU']:.2f}")
    if agg['avg_memEff_%'] == agg['avg_memEff_%']:
        print(f"  mean memEff:  {agg['avg_memEff_%']:.2f}%")
    if agg.get('max_mem_b'):
        print(f"  max memUsed: {fmt_bytes(agg['max_mem_b'])}")

    if args.csv:
        write_csv(rows, args.csv)

if __name__ == "__main__":
    main()
